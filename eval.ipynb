{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge Crack Detection using U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from keras import optimizers, metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "#import keras.metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import unet_crack as model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (256,256)\n",
    "data_dir = './samples/'\n",
    "namefile = 'test'\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.unetA(image_shape[0],image_shape[1],3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoint/bestmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_conf_mat(mask, gt_mask):\n",
    "    TP = TN = FP = FN = err = 0\n",
    "    #total_gt_pixels = sum(sum(gt_mask.astype(np.int16)))[0]\n",
    "    \n",
    "    for x in range(mask.shape[0]):\n",
    "        for y in range(mask.shape[1]):\n",
    "            if gt_mask[x][y] == True and mask[x][y] == True:\n",
    "                TP += 1\n",
    "            elif gt_mask[x][y] == False and mask[x][y] == True:\n",
    "                FP += 1\n",
    "            elif gt_mask[x][y] == True and mask[x][y] == False:\n",
    "                FN += 1\n",
    "            elif gt_mask[x][y] == False and mask[x][y] == False:\n",
    "                TN += 1\n",
    "            else:\n",
    "                err += 1\n",
    "                \n",
    "    if err > 0:\n",
    "        print(\"error pixels: {}\".format(err))\n",
    "    \n",
    "    conf_mat = np.array([[TP, FN], [FP, TN]])\n",
    "    #conf_mat_norm = np.array([[TP, FN], [FP, TN]])/total_gt_pixels\n",
    "    \n",
    "    if TP == 0:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f_score = 0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f_score = 2*precision*recall / (precision+recall)\n",
    "    \n",
    "    return conf_mat, precision, recall, f_score\n",
    "\n",
    "def scores(gt, mask):\n",
    "    y_true = np.reshape(gt, (gt.shape[0]*gt.shape[1], 1))\n",
    "    y_pred = np.reshape(mask, (mask.shape[0]*mask.shape[1], 1))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    Pr = tp/(tp+fp)\n",
    "    Re = tp/(tp+fn)\n",
    "    f1t = 2*(Pr*Re)/(Pr+Re)\n",
    "    iout = tp / (tp+fp+fn)\n",
    "    mcc = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    \n",
    "    return Pr, Re, f1t, iout, mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = data_dir #os.path.join(data_dir, 'test/')\n",
    "testfiles = glob(f\"{test_data_dir}/*.jpeg\")\n",
    "\n",
    "th_segment = 0.5\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For random testing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(testfiles))\n",
    "fname = os.path.basename(testfiles[i])\n",
    "\n",
    "img = cv2.imread(os.path.join(test_data_dir, fname))\n",
    "gt = cv2.imread(os.path.join(test_data_dir, fname[:-5]+'.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "y = model.predict(np.expand_dims(img, axis=0), batch_size=1)   \n",
    "tmp = y.squeeze()[:,:,0]\n",
    "mask = np.zeros(tmp.shape)\n",
    "idx = np.where(tmp < th_segment)\n",
    "mask[idx] = 1\n",
    "\n",
    "gt_mask = gt\n",
    "pred_mask = mask\n",
    "\n",
    "print(fname)\n",
    "\n",
    "#####\n",
    "\n",
    "plt.figure(figsize = (30,5))\n",
    "\n",
    "plt.subplot(151)\n",
    "plt.title('Raw Image')\n",
    "plt.imshow(img); plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.title('Ground Truth Mask')\n",
    "plt.imshow(gt_mask); plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.title('Predicted Mask')\n",
    "plt.imshow(pred_mask); plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.title('Pixel-level Confusion Matrix')\n",
    "#plt.xlabel(\"Predicted\")\n",
    "#plt.ylabel(\"Actual\")\n",
    "conf_mat_, precision_, recall_, f_score_ = measure_conf_mat(pred_mask, gt_mask)\n",
    "conf_mat_[1][1] = 0 # remove TN\n",
    "df_cm = pd.DataFrame(conf_mat_, index = [i for i in [\"True\", \"False\"]], columns = [i for i in [\"True\", \"False\"]])\n",
    "sn.heatmap(df_cm, annot=True, fmt='d')\n",
    "\n",
    "plt.subplot(155)\n",
    "Score = []\n",
    "Metric = []\n",
    "plt.title('Derivations from Confusion Matrix')\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "Score.append(precision_); Score.append(recall_); Score.append(f_score_);\n",
    "Metric = [\"Precision\", \"Recall\", \"F-Score\"]\n",
    "df_score = pd.DataFrame({\"Score\":Score, \"Metric\":Metric})\n",
    "splot = sn.barplot(x=\"Metric\",y=\"Score\",data=df_score)\n",
    "splot.set(ylim=(0, 1.0))\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.4f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=15,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conf_mat = np.zeros((2,2)).astype(np.int16)\n",
    "\n",
    "Precision = []\n",
    "Recall = []\n",
    "FScore = []\n",
    "IoU = []\n",
    "MCC = []\n",
    "\n",
    "img_count = 0\n",
    "\n",
    "for t in testfiles:\n",
    "    img = cv2.imread(t)\n",
    "    gt = cv2.imread(os.path.splitext(t)[0]+\".png\", cv2.IMREAD_GRAYSCALE)\n",
    "    y = model.predict(np.expand_dims(img, axis=0), batch_size=1)   \n",
    "    tmp = y.squeeze()[:,:,0]\n",
    "    mask = np.zeros(tmp.shape)\n",
    "    idx = np.where(tmp < th_segment)\n",
    "    mask[idx] = 1\n",
    "\n",
    "    precision_, recall_, f_score_, iout, mcc = scores(gt, mask)\n",
    "    Precision.append(precision_)\n",
    "    Recall.append(recall_)\n",
    "    FScore.append(f_score_)\n",
    "    IoU.append(iout)\n",
    "    MCC.append(mcc)\n",
    "\n",
    "    img_count += 1\n",
    "    if img_count % 100 == 0:\n",
    "        print(img_count)\n",
    "\n",
    "print(\"Average \\n - Precision: {:.4f}\\n - Recall: {:.4f}\\n - F-score: {:.4f}\\n - IoU: {:.4f}\\n - MCC: {:.4f}\".format(np.mean(Precision), np.mean(Recall), np.mean(FScore), np.mean(IoU), np.mean(MCC))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
